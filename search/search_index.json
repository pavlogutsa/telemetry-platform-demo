{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Telemetry Platform","text":"<p>This project simulates a remote monitoring / endpoint telemetry system:</p> <ul> <li>Devices send telemetry.</li> <li>We ingest, normalize, and persist it.</li> <li>We expose current and historical device state.</li> </ul> <p>This documentation describes architecture, data flow, APIs, infrastructure, security, and release roadmap.</p>"},{"location":"01-vision-scope.html","title":"Vision &amp; Scope","text":""},{"location":"01-vision-scope.html#purpose","title":"Purpose","text":"<p>The Telemetry Platform is a simulated endpoint-monitoring system. Its goal is to demonstrate a production-grade cloud backend architecture using Java 21, Spring Boot 3, Kafka, Redis, Oracle, and Kubernetes, packaged with full CI/CD and documentation pipelines.</p>"},{"location":"01-vision-scope.html#objectives","title":"Objectives","text":"<ul> <li>Collect and process large volumes of telemetry data from distributed device agents.</li> <li>Provide real-time and historical visibility into device health.</li> <li>Demonstrate fault-tolerant microservices and event-driven design.</li> <li>Showcase modern engineering practices:</li> <li>CI/CD with GitHub Actions</li> <li>Infrastructure as Code (Helm, Terraform)</li> <li>Observability and monitoring</li> <li>Automated testing (JUnit5, AssertJ, Mockito, Testcontainers)</li> <li>Documentation as code (MkDocs + GitHub Pages)</li> </ul>"},{"location":"01-vision-scope.html#out-of-scope","title":"Out of Scope","text":"<ul> <li>User interfaces or dashboards.</li> <li>Authentication federation or user management beyond API keys.</li> <li>Billing, tenant management, or analytics.</li> </ul>"},{"location":"01-vision-scope.html#stakeholders","title":"Stakeholders","text":"<ul> <li>Developers / DevOps engineers \u2013 exploring system integration.</li> <li>Hiring managers or reviewers \u2013 validating architectural and coding competence.</li> <li>Learners \u2013 experimenting with distributed Java stacks.</li> </ul>"},{"location":"01-vision-scope.html#success-criteria","title":"Success Criteria","text":"<ul> <li>All services deployable locally on Kubernetes or Oracle Cloud.</li> <li>Observable, tested, and documented system.</li> <li>Each release incrementally adds real functionality and complexity.</li> </ul>"},{"location":"02-architecture.html","title":"Architecture","text":"<p>The Telemetry Platform simulates a modern endpoint-monitoring system. It is built as a microservices architecture using Java 21 and Spring Boot 3, deployed on Kubernetes, with Kafka for messaging and Oracle DB for persistence.</p>"},{"location":"02-architecture.html#1-system-overview","title":"1. System Overview","text":"<p>The platform collects telemetry data (CPU, memory, disk, process list, etc.) from distributed device agents. Each agent periodically posts telemetry via REST; the data is normalized, streamed through Kafka, stored in Oracle for historical analysis, and exposed via REST APIs for operators or dashboards.</p> <p>This design showcases:</p> <ul> <li>Reactive, asynchronous message flow through Kafka </li> <li>Stateless ingestion and scalable processing  </li> <li>Deployment orchestration via Kubernetes + Helm </li> <li>Distributed coordination with Redis </li> <li>Observability using Prometheus + Grafana </li> <li>CI/CD and documentation as code via GitHub Actions + MkDocs</li> </ul>"},{"location":"02-architecture.html#2-high-level-architecture-diagram","title":"2. High-Level Architecture Diagram","text":"flowchart LR   D[\"Device Agent\"] --&gt; GW[\"NGINX Ingress / API Gateway\"]   GW --&gt; AI[\"agent-ingest-svc (REST to Kafka)\"]   AI --&gt; K[\"Kafka\"]   K --&gt; TP[\"telemetry-processor-svc\"]   TP --&gt; RD[\"Redis\"]   TP --&gt; OC[\"Oracle current\"]   TP --&gt; OH[\"Oracle history\"]   GW --&gt; DS[\"device-state-svc\"]   DS --&gt; OC   DS --&gt; OH   AI --&gt; PM[\"Prometheus\"]   TP --&gt; PM   DS --&gt; PM   PM --&gt; GF[\"Grafana\"]"},{"location":"02-architecture.html#3-component-breakdown","title":"3. Component Breakdown","text":""},{"location":"02-architecture.html#agent-ingest-svc","title":"agent-ingest-svc","text":"<p>Entry point for telemetry ingestion.</p> <ul> <li>Exposes <code>POST /telemetry</code>.</li> <li>Stateless Spring Boot 3 REST app producing to Kafka.</li> <li>Scales horizontally behind NGINX ingress.</li> </ul>"},{"location":"02-architecture.html#telemetry-processor-svc","title":"telemetry-processor-svc","text":"<p>Consumes <code>telemetry.raw</code> from Kafka.</p> <ul> <li>Normalizes and validates messages.</li> <li>Uses Redis for throttling and deduplication.</li> <li>Writes to Oracle tables: <code>device_status_current</code> and <code>device_status_history</code>.</li> <li>Publishes compacted topic <code>telemetry.latest</code> and append-only <code>telemetry.timeseries</code>.</li> </ul>"},{"location":"02-architecture.html#device-state-svc","title":"device-state-svc","text":"<p>Exposes device state via REST (<code>/devices/{id}/status</code>, <code>/history</code>).</p> <ul> <li>Reads Oracle tables; optional Redis cache.</li> <li>Provides JSON APIs for operators or dashboards.</li> </ul>"},{"location":"02-architecture.html#redis","title":"Redis","text":"<p>Distributed cache for deduplication and throttling windows.</p> <ul> <li>Future: read caching and API-level rate limiting.</li> </ul>"},{"location":"02-architecture.html#kafka","title":"Kafka","text":"<p>Backbone of event-driven communication.</p> <ul> <li>Topics: <code>telemetry.raw</code>, <code>telemetry.latest</code>, <code>telemetry.timeseries</code>.</li> <li>Schema Registry added in Release 4.</li> </ul>"},{"location":"02-architecture.html#oracle-db","title":"Oracle DB","text":"<p>Stores processed telemetry.</p> <ul> <li><code>device_status_current</code> \u2013 one record per device.</li> <li><code>device_status_history</code> \u2013 append-only time series.</li> <li>Indexed by <code>(device_id, timestamp)</code>.</li> </ul>"},{"location":"02-architecture.html#nginx-ingress-api-gateway","title":"NGINX Ingress / API Gateway","text":"<p>Routes traffic to backend services.</p> <ul> <li>Provides load balancing and TLS termination.</li> <li>Adds authentication headers from Release 5 onward.</li> </ul>"},{"location":"02-architecture.html#4-data-flow-summary","title":"4. Data Flow Summary","text":"<ol> <li>Device posts telemetry \u2192 agent-ingest-svc.</li> <li>agent-ingest-svc produces <code>telemetry.raw</code> to Kafka.</li> <li>telemetry-processor-svc consumes, throttles, writes to Oracle.</li> <li>device-state-svc exposes results via REST.</li> <li>Prometheus/Grafana collect runtime metrics.</li> </ol>"},{"location":"02-architecture.html#5-scalability-resilience","title":"5. Scalability &amp; Resilience","text":"<ul> <li>agent-ingest-svc scales via Kubernetes HPA.</li> <li>Kafka absorbs load spikes; decouples ingestion from processing.</li> <li>Redis prevents overload through throttling.</li> <li>Health and metrics exposed via Actuator endpoints.</li> <li>Rolling upgrades handled via Helm charts.</li> <li>Integration tests validate pipelines with Testcontainers.</li> </ul>"},{"location":"02-architecture.html#6-release-evolution","title":"6. Release Evolution","text":"Release Key Additions R1 Basic REST ingestion \u2192 Oracle direct R2 Introduced Kafka + telemetry-processor-svc R3 Added Redis throttling + history R4 Added Schema Registry + Debezium CDC R5 API-key auth + complete observability"},{"location":"03-data-design.html","title":"Data &amp; Messaging Design","text":""},{"location":"03-data-design.html#1-data-flow-overview","title":"1. Data Flow Overview","text":"<p>Telemetry data travels through multiple transformation layers:</p> Stage Producer Consumer Medium Description Raw Telemetry Device Agent agent-ingest-svc REST \u2192 Kafka Device sends JSON payload Normalized agent-ingest-svc telemetry-processor-svc Kafka topic <code>telemetry.raw</code> Standardized structure Latest State telemetry-processor-svc device-state-svc Kafka topic <code>telemetry.latest</code> Most recent record per device Time Series telemetry-processor-svc device-state-svc Kafka topic <code>telemetry.timeseries</code> Append-only history"},{"location":"03-data-design.html#2-kafka-topics","title":"2. Kafka Topics","text":""},{"location":"03-data-design.html#telemetryraw","title":"<code>telemetry.raw</code>","text":"<ul> <li>Unvalidated JSON or Avro messages received from devices.</li> <li>Schema defined in Apicurio (added in Release 4).</li> <li>Keyed by <code>deviceId</code>.</li> </ul>"},{"location":"03-data-design.html#telemetrylatest","title":"<code>telemetry.latest</code>","text":"<ul> <li>Compact-topic view (1 message per key).</li> <li>Used for quick \u201ccurrent status\u201d queries.</li> </ul>"},{"location":"03-data-design.html#telemetrytimeseries","title":"<code>telemetry.timeseries</code>","text":"<ul> <li>Append-only, chronological stream.</li> <li>Supports trend analysis and visual dashboards.</li> </ul>"},{"location":"03-data-design.html#3-oracle-database-schema","title":"3. Oracle Database Schema","text":""},{"location":"03-data-design.html#device_status_current","title":"<code>device_status_current</code>","text":"Column Type Description device_id VARCHAR2 Primary key cpu_pct NUMBER CPU usage mem_pct NUMBER Memory usage disk_alert BOOLEAN Disk health flag updated_at TIMESTAMP Last updated"},{"location":"03-data-design.html#device_status_history","title":"<code>device_status_history</code>","text":"Column Type Description device_id VARCHAR2 Device reference ts TIMESTAMP Timestamp cpu_pct NUMBER CPU usage mem_pct NUMBER Memory usage process_count NUMBER Count of running processes disk_state VARCHAR2 OK / ALERT PRIMARY INDEX (device_id, ts)"},{"location":"03-data-design.html#4-redis-usage","title":"4. Redis Usage","text":"<ul> <li>Throttling: Prevent high-frequency spam from devices.  </li> <li>Deduplication: Discard identical telemetry bursts within a short time window.  </li> <li>Read Cache (future): Cache recent device states to reduce DB reads.</li> </ul>"},{"location":"03-data-design.html#5-schema-registry-release-4","title":"5. Schema Registry (Release 4)","text":"<p>Apicurio Registry defines Avro schemas for <code>telemetry.*</code> topics to ensure producer/consumer compatibility.</p>"},{"location":"03-data-design.html#6-debezium-release-4","title":"6. Debezium (Release 4+)","text":"<p>Optional Oracle \u2192 Kafka change data capture for re-emitting DB updates downstream, enabling analytics pipelines.</p>"},{"location":"05-infrastructure.html","title":"Infrastructure","text":""},{"location":"05-infrastructure.html#1-local-deployment","title":"1. Local Deployment","text":"<p>All components deploy to a local Kubernetes cluster (Kind, k3d, or Minikube) via Helm charts under <code>/helm</code>.</p> Component Chart Path Purpose agent-ingest-svc <code>helm/agent-ingest-svc</code> REST ingestion service telemetry-processor-svc <code>helm/telemetry-processor-svc</code> Kafka consumer / DB writer device-state-svc <code>helm/device-state-svc</code> REST read service redis <code>helm/redis</code> Cache &amp; throttling kafka <code>helm/kafka</code> Event backbone oracle-xe <code>helm/oracle</code> Primary persistence nginx-ingress <code>helm/nginx-ingress</code> API gateway prometheus / grafana <code>helm/observability</code> Metrics stack observability <code>helm/observability</code> Prometheus + Grafana <p>Deploy locally:</p> <pre><code>helm dependency update ./helm\nhelm install telemetry ./helm\nkubectl port-forward svc/nginx-ingress-controller 8080:80\n</code></pre> <p>Access APIs through <code>http://localhost:8080/....</code></p>"},{"location":"05-infrastructure.html#2-containerization","title":"2. Containerization","text":"<p>Each service builds as a multi-arch Docker image using Spring Boot 3's buildpacks:</p> <pre><code>./gradlew bootBuildImage\n</code></pre> <p>Images run on both amd64 and arm64 (Apple Silicon). Helm values reference these tagged images (e.g., <code>ghcr.io/pavlogutsa/agent-ingest-svc:latest</code>).</p>"},{"location":"05-infrastructure.html#3-cicd-pipeline","title":"3. CI/CD Pipeline","text":"<p>GitHub Actions orchestrates:</p> <ul> <li>Build &amp; Test \u2013 Gradle compile, unit, and integration tests.</li> <li>Docker Build &amp; Push \u2013 buildpacks to registry.</li> <li>Helm Lint &amp; Template \u2013 validate charts.</li> <li>MkDocs Build \u2013 docs built &amp; deployed to Pages.</li> <li>(Optional) Terraform apply \u2192 OKE deployment.</li> </ul>"},{"location":"05-infrastructure.html#4-oracle-cloud-oci-deployment","title":"4. Oracle Cloud (OCI) Deployment","text":"<p>OCI free tier supports running this architecture using:</p> <ul> <li>Oracle Kubernetes Engine (OKE) for containers.</li> <li>Oracle Autonomous DB or containerized XE instance.</li> <li>Object Storage for backups.</li> </ul> <p>Terraform modules provision:</p> <ul> <li>VCN + subnets</li> <li>Node pools</li> <li>Load balancers</li> <li>Helm releases per component</li> </ul>"},{"location":"05-infrastructure.html#5-scaling-load-balancing","title":"5. Scaling &amp; Load Balancing","text":"<ul> <li>agent-ingest-svc \u2013 scales horizontally; ingress load-balances requests.</li> <li>Kafka \u2013 partitioned topics for parallelism.</li> <li>Redis \u2013 cluster mode for throughput.</li> <li>Oracle \u2013 partitioned tables, connection pool tuning.</li> <li>Prometheus + Grafana \u2013 scaled via stateful sets.</li> </ul>"},{"location":"05-infrastructure.html#6-backup-recovery","title":"6. Backup &amp; Recovery","text":"<ul> <li>Oracle export via <code>expdp</code> container job.</li> <li>Kafka topic retention policy: 7\u201314 days.</li> <li>Redis persistence optional (AOF).</li> <li>Helm values define storage classes for stateful components.</li> </ul>"},{"location":"05-infrastructure.html#7-future-infrastructure","title":"7. Future Infrastructure","text":"<ul> <li>CI/CD promotion pipeline (dev \u2192 staging \u2192 prod).</li> <li>Canary deployments via ingress annotations.</li> <li>Terraform modules for AWS EKS and GCP GKE equivalents.</li> </ul>"},{"location":"06-testing-strategy.html","title":"Testing Strategy","text":""},{"location":"06-testing-strategy.html#layers","title":"Layers","text":"<ol> <li>Unit tests</li> <li>JUnit 5</li> <li>AssertJ</li> <li> <p>Mockito for behavior testing</p> </li> <li> <p>Integration tests</p> </li> <li>Testcontainers:<ul> <li>Kafka / Redpanda</li> <li>Oracle XE</li> <li>Redis</li> </ul> </li> <li> <p>Validates that agent-ingest-svc -&gt; Kafka -&gt; telemetry-processor-svc -&gt; Oracle works</p> </li> <li> <p>End-to-end smoke</p> </li> <li>Run cluster locally (kind / k3d)</li> <li>Apply Helm charts</li> <li>POST /telemetry through ingress</li> <li>GET /devices/{id}/status</li> <li>Assert data path is correct</li> </ol>"},{"location":"06-testing-strategy.html#release-gating","title":"Release gating","text":"<p>Every release must pass end-to-end smoke.</p>"},{"location":"07-observability.html","title":"Observability","text":""},{"location":"07-observability.html#1-metrics","title":"1. Metrics","text":"<p>Each service exposes Spring Boot Actuator endpoints: - <code>/actuator/health</code> \u2192 readiness/liveness probes. - <code>/actuator/prometheus</code> \u2192 Prometheus scrape target.</p>"},{"location":"07-observability.html#key-metrics","title":"Key Metrics","text":"Service Metrics Examples agent-ingest-svc Request rate, response latency, Kafka produce time telemetry-processor-svc Kafka consumer lag, processed count, dropped messages device-state-svc Query latency, cache hits/misses Redis Memory usage, key eviction count Oracle Insert latency, connection pool usage"},{"location":"07-observability.html#2-prometheus-grafana","title":"2. Prometheus &amp; Grafana","text":"<p>Prometheus scrapes <code>/actuator/prometheus</code> from each service. Grafana visualizes: - Ingestion rate over time - DB insert latency - Kafka consumer lag - API response times - Resource utilization per pod</p> <p>Dashboards live under <code>helm/observability/grafana/dashboards</code>.</p>"},{"location":"07-observability.html#3-logging-tracing","title":"3. Logging &amp; Tracing","text":"<ul> <li>Structured JSON logs via Logback.</li> <li>Correlation ID (traceId) injected through HTTP headers.</li> <li>Future upgrade: OpenTelemetry for distributed tracing.</li> </ul>"},{"location":"07-observability.html#4-alerts","title":"4. Alerts","text":"<p>Prometheus rules: - High CPU / memory on pods - Kafka consumer lag threshold - Oracle latency above threshold - Service down alerts (no /health response)</p>"},{"location":"08-security.html","title":"Security &amp; Authentication","text":""},{"location":"08-security.html#1-overview","title":"1. Overview","text":"<p>Security is introduced progressively through releases:</p> Release Security Feature R1\u2013R2 Internal only, no authentication R3 TLS via NGINX Ingress R4 Secure secrets handling in Kubernetes R5 API-key authentication and role-based access"},{"location":"08-security.html#2-api-gateway-authentication-release-5","title":"2. API Gateway Authentication (Release 5)","text":""},{"location":"08-security.html#flow","title":"Flow","text":"<ol> <li>Client sends requests with an API key header:</li> </ol> <pre><code>X-API-Key: &lt;token&gt;\n</code></pre> <ol> <li>NGINX Ingress validates the key against a ConfigMap or Redis store.  </li> <li>The ingress injects role metadata as headers:</li> <li><code>X-Caller-Role: ingest</code></li> <li><code>X-Caller-Role: read</code></li> <li>Downstream services authorize based on the header.</li> </ol>"},{"location":"08-security.html#roles","title":"Roles","text":"Role Permissions ingest Access <code>/telemetry</code> read Access <code>/devices/**</code> admin Access metrics and config endpoints <p>API keys can be rotated via ConfigMap update or REST management endpoint.</p>"},{"location":"08-security.html#3-service-to-service-security","title":"3. Service-to-Service Security","text":"<ul> <li>Kafka: SASL/SSL authentication for producers and consumers.  </li> <li>Redis: password-protected, limited network access via K8s NetworkPolicy.  </li> <li>Oracle: JDBC SSL enabled; credentials from Secrets.  </li> <li>Each service uses least-privilege principle in its connection configuration.</li> </ul>"},{"location":"08-security.html#4-secrets-management","title":"4. Secrets Management","text":"<ul> <li>All credentials (DB, Kafka, Redis, API-keys) stored in Kubernetes Secrets.  </li> <li>Gradle builds inject environment variables via CI pipeline.  </li> <li>Example secret manifest:</li> </ul> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: oracle-credentials\ntype: Opaque\ndata:\n  username: b3JhY2xlX3VzZXI=\n  password: c2VjdXJlcGFzcw==\n</code></pre>"},{"location":"08-security.html#5-transport-security","title":"5. Transport Security","text":"<ul> <li>NGINX Ingress terminates HTTPS using Let's Encrypt or self-signed TLS.</li> <li>Internal pod-to-pod communication restricted to cluster network.</li> <li>Future: mutual TLS between internal microservices.</li> </ul>"},{"location":"08-security.html#6-logging-audit","title":"6. Logging &amp; Audit","text":"<ul> <li>All API calls logged with request ID, user, and role.</li> <li>Audit log shipped to centralized storage (Elastic / Loki).</li> <li>Suspicious or repeated failed requests trigger alerts.</li> </ul>"},{"location":"08-security.html#7-future-enhancements","title":"7. Future Enhancements","text":"<ul> <li>OIDC / JWT token-based authentication.</li> <li>Fine-grained RBAC at service level.</li> <li>Integration with Vault for dynamic secret rotation.</li> <li>Signed telemetry payloads using public/private key pairs on agents.</li> <li>NetworkPolicy hardening for zero-trust deployment.</li> </ul>"},{"location":"09-release-roadmap.html","title":"Release Roadmap","text":""},{"location":"09-release-roadmap.html#release-1-mvp","title":"Release 1 \u2013 MVP","text":"<ul> <li>agent-ingest-svc \u2192 Oracle direct write.</li> <li>device-state-svc \u2192 Oracle read.</li> <li>Helm deployment + Prometheus integration.</li> <li>GitHub Actions CI pipeline.</li> <li>Documentation published via MkDocs.</li> </ul>"},{"location":"09-release-roadmap.html#release-2-event-backbone","title":"Release 2 \u2013 Event Backbone","text":"<ul> <li>Introduce Kafka cluster.</li> <li>telemetry-processor-svc consumes <code>telemetry.raw</code>.</li> <li>agent-ingest-svc produces to Kafka.</li> <li>device-state-svc decoupled from writes.</li> </ul>"},{"location":"09-release-roadmap.html#release-3-redis-throttling-history","title":"Release 3 \u2013 Redis Throttling &amp; History","text":"<ul> <li>Add Redis distributed cache.</li> <li>telemetry-processor-svc deduplicates noisy agents.</li> <li>Introduce <code>telemetry.latest</code> &amp; <code>telemetry.timeseries</code>.</li> <li>Historical API <code>/devices/{id}/history</code>.</li> </ul>"},{"location":"09-release-roadmap.html#release-4-schema-registry-observability","title":"Release 4 \u2013 Schema Registry &amp; Observability","text":"<ul> <li>Apicurio Registry for Avro schemas.</li> <li>Debezium CDC from Oracle \u2192 Kafka.</li> <li>Enhanced Prometheus/Grafana dashboards.</li> <li>Helm chart linting &amp; automated tests.</li> </ul>"},{"location":"09-release-roadmap.html#release-5-authentication-final-polish","title":"Release 5 \u2013 Authentication &amp; Final Polish","text":"<ul> <li>NGINX ingress with API key enforcement.</li> <li>Full metrics &amp; alerting coverage.</li> <li>MkDocs site complete with diagrams &amp; changelog.</li> <li>Optional OKE deployment pipeline.</li> </ul>"},{"location":"09-release-roadmap.html#future-ideas","title":"Future Ideas","text":"<ul> <li>OIDC / JWT-based auth.</li> <li>GraphQL read API.</li> <li>Multi-tenant support.</li> <li>Integration with ClickHouse or Elastic for analytics.</li> <li>Synthetic load generator for performance testing.</li> </ul>"},{"location":"10-future-notes.html","title":"Future Notes","text":"<ul> <li>Partitioning / archiving strategy for device_status_history</li> <li>Debezium CDC from Oracle into Kafka</li> <li>Multi-tenant separation (orgId per device)</li> <li>JWT/OIDC validation at ingress instead of static API keys</li> </ul>"},{"location":"11-release1-spec.html","title":"Telemetry Platform \u2013 Release 1 Implementation Spec (Code &amp; Kubernetes)","text":"<p>This document defines all source code, configuration, and deployment files required for Release 1 of the Telemetry Platform project.</p>"},{"location":"11-release1-spec.html#1-goal","title":"1. Goal","text":"<ul> <li>Devices send telemetry via HTTP.</li> <li><code>agent-ingest-svc</code> receives telemetry and writes the latest device state into Oracle DB.</li> <li><code>device-state-svc</code> exposes a REST API to read that latest device state.</li> <li>Both services run in a local Kubernetes cluster.</li> <li>NGINX Ingress exposes:</li> <li><code>POST /telemetry</code> \u2192 <code>agent-ingest-svc</code></li> <li><code>GET /devices/{deviceId}/status</code> \u2192 <code>device-state-svc</code></li> <li><code>/actuator/health</code> and <code>/actuator/prometheus</code> are enabled for both services.</li> </ul> <p>No Kafka, no Redis yet.</p>"},{"location":"11-release1-spec.html#2-repository-layout","title":"2. Repository Layout","text":"<p>Ensure the following structure under repo root <code>telemetry-platform-demo/</code>:</p> <pre><code>telemetry-platform-demo/\n  agent-ingest-svc/\n  device-state-svc/\n  docs/\n    11-release1-spec.md\n  helm/\n    telemetry-platform/\n  kind-config.yaml\n  README.md\n  build.gradle\n  settings.gradle\n</code></pre>"},{"location":"11-release1-spec.html#3-technology-stack","title":"3. Technology Stack","text":"<ul> <li>Java 21</li> <li>Spring Boot 3.3.5</li> <li>Gradle (Groovy DSL, multi-module project)</li> <li>Oracle XE</li> <li>Spring Web + JDBC + Actuator</li> <li>Docker images</li> <li>Kubernetes manifests (YAML)</li> <li>NGINX Ingress Controller</li> </ul>"},{"location":"11-release1-spec.html#4-database-design-oracle","title":"4. Database Design (Oracle)","text":""},{"location":"11-release1-spec.html#41-schema-user","title":"4.1. Schema / User","text":"<p>Assumptions (created manually outside of this spec):</p> <ul> <li>Oracle XE running with PDB/service name XEPDB1</li> <li>DB user: telemetry</li> <li>Password: telemetry_pw</li> <li>JDBC URL (inside cluster):</li> </ul> <pre><code>jdbc:oracle:thin:@oracle-db.telemetry.svc.cluster.local:1521/XEPDB1\n</code></pre>"},{"location":"11-release1-spec.html#42-table-definition","title":"4.2. Table Definition","text":"<p>The table used in Release 1 to store latest device status:</p> <pre><code>CREATE TABLE telemetry.device_status_current (\n    device_id   VARCHAR2(128) PRIMARY KEY,\n    cpu_pct     NUMBER(5,2),\n    mem_pct     NUMBER(5,2),\n    disk_alert  CHAR(1),\n    updated_at  TIMESTAMP\n);\n</code></pre> <p>Semantics:</p> <ul> <li>One row per device_id</li> <li>cpu_pct, mem_pct = current CPU/memory usage</li> <li>disk_alert = 'Y' if some disk alert is active, 'N' otherwise</li> <li>updated_at = last update time (DB time)</li> </ul>"},{"location":"11-release1-spec.html#5-shared-spring-configuration-both-services","title":"5. Shared Spring Configuration (Both Services)","text":"<p>Both agent-ingest-svc and device-state-svc must:</p> <ul> <li>Use Spring Boot 3 + Java 21</li> <li>Use Spring JDBC Template</li> <li>Read DB connection from environment variables:</li> <li>ORACLE_JDBC_URL</li> <li>ORACLE_USER</li> <li>ORACLE_PASSWORD</li> <li>Expose:</li> <li><code>/actuator/health</code></li> <li><code>/actuator/prometheus</code></li> </ul>"},{"location":"11-release1-spec.html#51-shared-gradle-build-configuration","title":"5.1. Shared Gradle Build Configuration","text":"<p>The project uses a multi-module Gradle structure with Groovy DSL (<code>build.gradle</code>). Plugin versions are managed at the root level.</p> <p>Root <code>build.gradle</code>:</p> <pre><code>plugins {\n    id 'org.springframework.boot' version '3.3.5' apply false\n    id 'io.spring.dependency-management' version '1.1.5' apply false\n    id 'java'\n}\n\nallprojects {\n    group = 'com.telemetry'\n    version = '0.0.1-SNAPSHOT'\n\n    repositories {\n        mavenCentral()\n    }\n}\n\nsubprojects {\n    apply plugin: 'java'\n\n    java {\n        sourceCompatibility = JavaVersion.VERSION_21\n    }\n\n    tasks.withType(Test).configureEach {\n        useJUnitPlatform()\n    }\n}\n</code></pre> <p>Service-level <code>build.gradle</code> (e.g., <code>agent-ingest-svc/build.gradle</code>):</p> <pre><code>plugins {\n    id(\"org.springframework.boot\")\n    id(\"io.spring.dependency-management\")\n    id(\"java\")\n}\n\njava {\n    toolchain {\n        languageVersion.set(JavaLanguageVersion.of(21))\n    }\n}\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    implementation(\"org.springframework.boot:spring-boot-starter-web\")\n    implementation(\"org.springframework.boot:spring-boot-starter-actuator\")\n    implementation(\"org.springframework.boot:spring-boot-starter-jdbc\")\n    implementation(\"com.oracle.database.jdbc:ojdbc11:23.4.0.24.05\")\n\n    testImplementation(\"org.springframework.boot:spring-boot-starter-test\")\n    testImplementation(\"org.assertj:assertj-core:3.26.0\")\n    testImplementation(\"org.mockito:mockito-core:5.12.0\")\n}\n</code></pre>"},{"location":"11-release1-spec.html#52-shared-applicationyaml-template","title":"5.2. Shared application.yaml Template","text":"<p>Each service has <code>src/main/resources/application.yaml</code>:</p> <pre><code>server:\n  port: 8080\n\nspring:\n  datasource:\n    url: ${ORACLE_JDBC_URL}\n    username: ${ORACLE_USER}\n    password: ${ORACLE_PASSWORD}\n    driver-class-name: oracle.jdbc.OracleDriver\n\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: health,metrics,prometheus\n  endpoint:\n    prometheus:\n      enabled: true\n</code></pre> <p>Note: - Services may use different ports (e.g., agent-ingest-svc: 8080, device-state-svc: 8081) for local development. In Kubernetes, services run in separate pods and can use the same port. - Kafka configuration may be present in <code>application.yaml</code> but is not used in Release 1.</p>"},{"location":"11-release1-spec.html#6-service-1-agent-ingest-svc","title":"6. Service 1 \u2013 agent-ingest-svc","text":""},{"location":"11-release1-spec.html#61-purpose","title":"6.1. Purpose","text":"<ul> <li>Receive telemetry data from devices via HTTP (JSON).</li> <li>Persist latest telemetry snapshot per device into telemetry.device_status_current.</li> <li>Overwrite existing row for that device_id (upsert semantics).</li> <li>HTTP endpoint inside the service: <code>POST /telemetry</code>.</li> <li>Externally exposed via Ingress as <code>POST /telemetry</code>.</li> </ul>"},{"location":"11-release1-spec.html#62-project-structure","title":"6.2. Project Structure","text":"<p>Under <code>agent-ingest-svc/</code>:</p> <pre><code>agent-ingest-svc/\n  build.gradle\n  src/\n    main/\n      java/\n        com/telemetry/agent/ingest/\n          AgentIngestApplication.java\n          api/TelemetryController.java\n          api/TelemetryRequest.java\n          repo/DeviceStatusRepository.java\n      resources/\n        application.yaml\n</code></pre>"},{"location":"11-release1-spec.html#63-main-application-class","title":"6.3. Main Application Class","text":"<p><code>src/main/java/com/telemetry/agent/ingest/AgentIngestApplication.java</code>:</p> <pre><code>package com.telemetry.agent.ingest;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class AgentIngestApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(AgentIngestApplication.class, args);\n    }\n}\n</code></pre>"},{"location":"11-release1-spec.html#64-dto-telemetryrequest","title":"6.4. DTO \u2013 TelemetryRequest","text":"<p><code>src/main/java/com/telemetry/agent/ingest/TelemetryRequest.java</code>:</p> <pre><code>package com.telemetry.agent.ingest.api;\n\nimport java.time.Instant;\nimport java.util.List;\n\npublic record TelemetryRequest(\n        String deviceId,\n        double cpu,\n        double mem,\n        Boolean diskAlert,\n        Instant timestamp,\n        List&lt;ProcessSample&gt; processes\n) {\n    public record ProcessSample(\n            String name,\n            double cpu,\n            double mem\n    ) {}\n}\n</code></pre> <p>Requirements:</p> <ul> <li>JSON body from clients maps to this record.</li> <li>diskAlert may be null (default treat as false).</li> </ul>"},{"location":"11-release1-spec.html#65-repository-devicestatusrepository","title":"6.5. Repository \u2013 DeviceStatusRepository","text":"<p><code>src/main/java/com/telemetry/agent/ingest/repo/DeviceStatusRepository.java</code>:</p> <pre><code>package com.telemetry.agent.ingest.repo;\n\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport org.springframework.stereotype.Repository;\n\nimport java.time.Instant;\n\n@Repository\npublic class DeviceStatusRepository {\n\n    private final JdbcTemplate jdbc;\n\n    public DeviceStatusRepository(JdbcTemplate jdbc) {\n        this.jdbc = jdbc;\n    }\n\n    public void upsertStatus(String deviceId,\n                             double cpuPct,\n                             double memPct,\n                             boolean diskAlert,\n                             Instant timestamp) {\n\n        int updated = jdbc.update(\"\"\"\n            UPDATE telemetry.device_status_current\n               SET cpu_pct = ?, mem_pct = ?, disk_alert = ?, updated_at = SYSTIMESTAMP\n             WHERE device_id = ?\n        \"\"\", cpuPct, memPct, diskAlert ? \"Y\" : \"N\", deviceId);\n\n        if (updated == 0) {\n            jdbc.update(\"\"\"\n                INSERT INTO telemetry.device_status_current\n                    (device_id, cpu_pct, mem_pct, disk_alert, updated_at)\n                VALUES (?, ?, ?, ?, SYSTIMESTAMP)\n            \"\"\", deviceId, cpuPct, memPct, diskAlert ? \"Y\" : \"N\");\n        }\n    }\n}\n</code></pre>"},{"location":"11-release1-spec.html#66-controller-telemetrycontroller","title":"6.6. Controller \u2013 TelemetryController","text":"<p><code>src/main/java/com/telemetry/agent/ingest/TelemetryController.java</code>:</p> <pre><code>package com.telemetry.agent.ingest.api;\n\nimport com.telemetry.agent.ingest.repo.DeviceStatusRepository;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.web.bind.annotation.*;\n\n@RestController\n@RequestMapping(\"/telemetry\")\npublic class TelemetryController {\n\n    private final DeviceStatusRepository repo;\n\n    public TelemetryController(DeviceStatusRepository repo) {\n        this.repo = repo;\n    }\n\n    @PostMapping\n    public ResponseEntity&lt;Void&gt; postTelemetry(@RequestBody TelemetryRequest body) {\n        if (body.deviceId() == null || body.deviceId().isBlank()) {\n            return ResponseEntity.badRequest().build();\n        }\n\n        repo.upsertStatus(\n                body.deviceId(),\n                body.cpu(),\n                body.mem(),\n                body.diskAlert() != null &amp;&amp; body.diskAlert(),\n                body.timestamp()\n        );\n\n        return ResponseEntity.accepted().build();\n    }\n}\n</code></pre> <p>Behavior:</p> <ul> <li>If deviceId is missing or blank \u2192 HTTP 400.</li> <li>Otherwise:</li> <li>Upsert row in device_status_current.</li> <li>Return HTTP 202 Accepted.</li> </ul>"},{"location":"11-release1-spec.html#67-integration-tests","title":"6.7. Integration Tests","text":"<ul> <li>Location: <code>agent-ingest-svc/src/test/java/com/telemetry/agent/ingest/TelemetryControllerIntegrationTest.java</code></li> <li><code>postTelemetry_insertsOrUpdatesRowInDb</code> boots the full Spring context (profile <code>test</code>), posts JSON to <code>/telemetry</code>, and asserts the row is persisted in <code>telemetry.device_status_current</code>.</li> <li><code>postTelemetry_withoutDeviceId_returnsBadRequestAndDoesNotInsert</code> verifies a malformed request returns HTTP 400 and leaves the table empty.</li> </ul>"},{"location":"11-release1-spec.html#7-service-2-device-state-svc","title":"7. Service 2 \u2013 device-state-svc","text":""},{"location":"11-release1-spec.html#71-purpose","title":"7.1. Purpose","text":"<ul> <li>Read the latest device status from telemetry.device_status_current.</li> <li>Expose REST endpoint:</li> <li>Internal: <code>GET /devices/{deviceId}/status</code></li> <li>Via Ingress: <code>GET /devices/{deviceId}/status</code>.</li> </ul>"},{"location":"11-release1-spec.html#72-project-structure","title":"7.2. Project Structure","text":"<p>Under <code>device-state-svc/</code>:</p> <pre><code>device-state-svc/\n  build.gradle\n  src/\n    main/\n      java/\n        com/telemetry/state/\n          DeviceStateApplication.java\n          controller/DeviceStatusController.java\n          repo/DeviceStatusReadRepository.java\n      resources/\n        application.yaml\n</code></pre>"},{"location":"11-release1-spec.html#73-main-application-class","title":"7.3. Main Application Class","text":"<p><code>src/main/java/com/telemetry/state/DeviceStateApplication.java</code>:</p> <pre><code>package com.telemetry.state;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class DeviceStateApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(DeviceStateApplication.class, args);\n    }\n}\n</code></pre>"},{"location":"11-release1-spec.html#74-repository-devicestatusreadrepository","title":"7.4. Repository \u2013 DeviceStatusReadRepository","text":"<p><code>src/main/java/com/telemetry/state/repo/DeviceStatusReadRepository.java</code>:</p> <pre><code>package com.telemetry.state.repo;\n\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport org.springframework.stereotype.Repository;\n\nimport java.time.Instant;\nimport java.util.Optional;\n\n@Repository\npublic class DeviceStatusReadRepository {\n\n    private final JdbcTemplate jdbc;\n\n    public DeviceStatusReadRepository(JdbcTemplate jdbc) {\n        this.jdbc = jdbc;\n    }\n\n    public Optional&lt;DeviceStatus&gt; getStatus(String deviceId) {\n        return jdbc.query(\"\"\"\n            SELECT device_id, cpu_pct, mem_pct, disk_alert, updated_at\n              FROM telemetry.device_status_current\n             WHERE device_id = ?\n        \"\"\", rs -&gt; {\n            if (!rs.next()) {\n                return Optional.empty();\n            }\n            return Optional.of(new DeviceStatus(\n                    rs.getString(\"device_id\"),\n                    rs.getDouble(\"cpu_pct\"),\n                    rs.getDouble(\"mem_pct\"),\n                    \"Y\".equals(rs.getString(\"disk_alert\")),\n                    rs.getTimestamp(\"updated_at\").toInstant()\n            ));\n        }, deviceId);\n    }\n\n    public record DeviceStatus(\n            String deviceId,\n            double cpuPct,\n            double memPct,\n            boolean diskAlert,\n            Instant updatedAt\n    ) {}\n}\n</code></pre>"},{"location":"11-release1-spec.html#75-controller-devicestatuscontroller","title":"7.5. Controller \u2013 DeviceStatusController","text":"<p><code>src/main/java/com/telemetry/state/controller/DeviceStatusController.java</code>:</p> <pre><code>package com.telemetry.state.controller;\n\nimport com.telemetry.state.repo.DeviceStatusReadRepository;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.web.bind.annotation.*;\n\n@RestController\n@RequestMapping(\"/devices\")\npublic class DeviceStatusController {\n\n    private final DeviceStatusReadRepository repo;\n\n    public DeviceStatusController(DeviceStatusReadRepository repo) {\n        this.repo = repo;\n    }\n\n    @GetMapping(\"/{deviceId}/status\")\n    public ResponseEntity&lt;?&gt; getStatus(@PathVariable String deviceId) {\n        return repo.getStatus(deviceId)\n                .&lt;ResponseEntity&lt;?&gt;&gt;map(ResponseEntity::ok)\n                .orElseGet(() -&gt; ResponseEntity.notFound().build());\n    }\n}\n</code></pre> <p>Behavior:</p> <ul> <li>If row exists \u2192 HTTP 200 + JSON body of DeviceStatus.</li> <li>If not \u2192 HTTP 404.</li> </ul>"},{"location":"11-release1-spec.html#76-integration-tests","title":"7.6. Integration Tests","text":"<ul> <li>Location: <code>device-state-svc/src/test/java/com/telemetry/state/controller/DeviceStatusControllerIntegrationTest.java</code></li> <li><code>getStatus_existingDevice_returnsStatusJson</code> loads the Spring context, seeds <code>telemetry.device_status_current</code>, and asserts <code>/devices/{deviceId}/status</code> returns HTTP 200 with the serialized JSON payload.</li> <li><code>getStatus_unknownDevice_returns404</code> ensures an empty database results in HTTP 404 when the device is not found.</li> </ul>"},{"location":"11-release1-spec.html#8-dockerfiles","title":"8. Dockerfiles","text":"<p>Both services are packaged into Docker images.</p>"},{"location":"11-release1-spec.html#81-agent-ingest-svcdockerfile","title":"8.1. agent-ingest-svc/Dockerfile","text":"<pre><code>FROM eclipse-temurin:21-jre\nWORKDIR /app\nCOPY build/libs/*.jar app.jar\nENV JAVA_OPTS=\"\"\nENTRYPOINT [\"sh\", \"-c\", \"java $JAVA_OPTS -jar app.jar\"]\n</code></pre>"},{"location":"11-release1-spec.html#82-device-state-svcdockerfile","title":"8.2. device-state-svc/Dockerfile","text":"<pre><code>FROM eclipse-temurin:21-jre\nWORKDIR /app\nCOPY build/libs/*.jar app.jar\nENV JAVA_OPTS=\"\"\nENTRYPOINT [\"sh\", \"-c\", \"java $JAVA_OPTS -jar app.jar\"]\n</code></pre>"},{"location":"11-release1-spec.html#9-kubernetes-manifests","title":"9. Kubernetes Manifests","text":"<p>All manifests target namespace telemetry.</p>"},{"location":"11-release1-spec.html#91-secret-for-oracle-credentials","title":"9.1. Secret for Oracle Credentials","text":"<p><code>k8s/oracle/oracle-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: oracle-credentials\n  namespace: telemetry\ntype: Opaque\nstringData:\n  ORACLE_PASSWORD: telemetry_pw\n  ORACLE_USER: telemetry\n  ORACLE_JDBC_URL: jdbc:oracle:thin:@oracle-db.telemetry.svc.cluster.local:1521/XEPDB1\n</code></pre>"},{"location":"11-release1-spec.html#92-oracle-statefulset-service","title":"9.2. Oracle StatefulSet &amp; Service","text":"<p><code>k8s/oracle/oracle-statefulset.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: oracle-db\n  namespace: telemetry\nspec:\n  ports:\n    - name: sql\n      port: 1521\n      targetPort: 1521\n  clusterIP: None\n  selector:\n    app: oracle-db\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: oracle-db\n  namespace: telemetry\nspec:\n  serviceName: oracle-db\n  replicas: 1\n  selector:\n    matchLabels:\n      app: oracle-db\n  template:\n    metadata:\n      labels:\n        app: oracle-db\n    spec:\n      containers:\n        - name: oracle\n          image: gvenzl/oracle-xe:21-slim\n          env:\n            - name: ORACLE_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: oracle-credentials\n                  key: ORACLE_PASSWORD\n          ports:\n            - containerPort: 1521\n              name: sql\n          volumeMounts:\n            - name: oracle-data\n              mountPath: /opt/oracle/oradata\n      volumes:\n        - name: oracle-data\n          emptyDir: {}\n</code></pre> <p>Note: user/schema/table creation is done separately (via sqlplus).</p>"},{"location":"11-release1-spec.html#93-agent-ingest-svc-chart-deployment-service","title":"9.3. agent-ingest-svc Chart (Deployment &amp; Service)","text":"<p><code>helm/telemetry-platform/charts/agent-ingest-svc/templates/deployment.yaml</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: agent-ingest-svc\n  namespace: telemetry\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: agent-ingest-svc\n  template:\n    metadata:\n      labels:\n        app: agent-ingest-svc\n    spec:\n      containers:\n        - name: agent-ingest-svc\n          image: agent-ingest-svc:release1\n          imagePullPolicy: IfNotPresent\n          envFrom:\n            - secretRef:\n                name: oracle-credentials\n          ports:\n            - containerPort: 8080\n              name: http\n          readinessProbe:\n            httpGet:\n              path: /actuator/health\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: agent-ingest-svc\n  namespace: telemetry\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/path: \"/actuator/prometheus\"\n    prometheus.io/port: \"8080\"\nspec:\n  selector:\n    app: agent-ingest-svc\n  ports:\n    - port: 8080\n          targetPort: 8080\n      name: http\n</code></pre>"},{"location":"11-release1-spec.html#94-device-state-svc-chart-deployment-service","title":"9.4. device-state-svc Chart (Deployment &amp; Service)","text":"<p><code>helm/telemetry-platform/charts/device-state-svc/templates/deployment.yaml</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: device-state-svc\n  namespace: telemetry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: device-state-svc\n  template:\n    metadata:\n      labels:\n        app: device-state-svc\n    spec:\n      containers:\n        - name: device-state-svc\n          image: device-state-svc:release1\n          imagePullPolicy: IfNotPresent\n          envFrom:\n            - secretRef:\n                name: oracle-credentials\n          ports:\n            - containerPort: 8081\n              name: http\n          readinessProbe:\n            httpGet:\n              path: /actuator/health\n              port: 8081\n            initialDelaySeconds: 5\n            periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: device-state-svc\n  namespace: telemetry\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/path: \"/actuator/prometheus\"\n    prometheus.io/port: \"8080\"\nspec:\n  selector:\n    app: device-state-svc\n  ports:\n    - port: 8080\n      targetPort: 8081\n      name: http\n</code></pre>"},{"location":"11-release1-spec.html#95-ingress-nginx-api-gateway-chart","title":"9.5. Ingress \u2013 NGINX API Gateway Chart","text":"<p><code>helm/telemetry-platform/charts/ingress/templates/ingress.yaml</code>:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: telemetry-gateway\n  namespace: telemetry\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: telemetry.internal\n      http:\n        paths:\n          - path: /telemetry/?(.*)\n            pathType: Prefix\n            backend:\n              service:\n                name: agent-ingest-svc\n                port:\n                  number: 8080\n          - path: /devices/?(.*)\n            pathType: Prefix\n            backend:\n              service:\n                name: device-state-svc\n                port:\n                  number: 8080\n</code></pre> <p>Behavior:</p> <ul> <li>External <code>POST /telemetry</code> \u2192 agent-ingest-svc <code>POST /telemetry</code></li> <li>External <code>GET /devices/{id}/status</code> \u2192 device-state-svc <code>GET /devices/{id}/status</code></li> </ul>"},{"location":"11-release1-spec.html#96-helm-chart-structure","title":"9.6. Helm Chart Structure","text":"<p>Helm charts are provided under <code>helm/telemetry-platform</code> to deploy the entire stack:</p> <ul> <li>Root chart (<code>Chart.yaml</code>) aggregates four dependencies: <code>oracle</code>, <code>agent-ingest-svc</code>, <code>device-state-svc</code>, and <code>ingress</code>.</li> <li>Global values (<code>values.yaml</code>) provide shared settings:</li> <li><code>global.namespace</code> = <code>telemetry</code></li> <li><code>global.oracle</code> = default Oracle credentials and JDBC URL (overridable per environment)</li> <li>Dependency overrides use the dependency names (<code>agent-ingest-svc</code>, <code>device-state-svc</code>, <code>ingress</code>) to set images, replica counts, ports, and host.</li> <li>Oracle subchart (<code>charts/oracle</code>) provisions:</li> <li><code>Secret</code> (<code>oracle-credentials</code>) using global credentials</li> <li>Optional PVC (enabled by default, size 10Gi)</li> <li>StatefulSet <code>oracle-db</code> using image <code>gvenzl/oracle-xe:21-slim</code></li> <li>Service subcharts (<code>charts/agent-ingest-svc</code>, <code>charts/device-state-svc</code>) create Deployments and Services that mirror the Kubernetes manifests in sections 9.3 and 9.4. Notable defaults:</li> <li><code>agent-ingest-svc</code>: replicas=2, container port 8080</li> <li><code>device-state-svc</code>: replicas=1, container port 8081 (Service still exposed on port 8080, targeting 8081)</li> <li>Ingress subchart (<code>charts/ingress</code>) exposes the API via host <code>telemetry.internal</code> with the same path routing as section 9.5.</li> <li>Install example:</li> </ul> <pre><code>helm upgrade --install telemetry-platform ./helm/telemetry-platform \\\n  --namespace telemetry \\\n  --create-namespace\n</code></pre> <p>Override values (for example, container images) using <code>-f custom-values.yaml</code> or <code>--set dependencyName.image=...</code>.</p>"},{"location":"11-release1-spec.html#10-local-kubernetes-deployment-sanity-check","title":"10. Local Kubernetes Deployment (Sanity Check)","text":"<p>This flow mirrors the contributor quickstart in <code>README.md</code>.</p> <ol> <li> <p>Build the services <pre><code>./gradlew clean build\n</code></pre></p> </li> <li> <p>Build Docker images <pre><code>docker build -t agent-ingest-svc:release1 ./agent-ingest-svc\ndocker build -t device-state-svc:release1 ./device-state-svc\n</code></pre></p> </li> <li> <p>Create Kind cluster with ingress ports <pre><code>kind create cluster --name telemetry --config kind-config.yaml\n</code></pre></p> </li> <li> <p>Load images into Kind <pre><code>kind load docker-image agent-ingest-svc:release1 --name telemetry\nkind load docker-image device-state-svc:release1 --name telemetry\n</code></pre></p> </li> <li> <p>Install NGINX Ingress (if absent) <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\n</code></pre></p> </li> <li> <p>Deploy with Helm <pre><code>helm dependency update helm/telemetry-platform\nhelm upgrade --install telemetry-platform ./helm/telemetry-platform \\\n  --namespace telemetry \\\n  --create-namespace\n</code></pre></p> </li> <li> <p>Map ingress host locally <pre><code>127.0.0.1 telemetry.internal\n</code></pre>    Add the entry above to <code>/etc/hosts</code>.</p> </li> <li> <p>Smoke-test the APIs <pre><code>curl -X POST http://telemetry.internal/telemetry \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"deviceId\": \"laptop-4421\",\n        \"cpu\": 0.82,\n        \"mem\": 0.73,\n        \"diskAlert\": false,\n        \"timestamp\": \"2025-11-02T18:22:00Z\",\n        \"processes\": []\n      }'\n\ncurl http://telemetry.internal/devices/laptop-4421/status\n</code></pre></p> </li> </ol> <p>Expect HTTP 200 with device telemetry JSON.</p> <ol> <li>Automated reset &amp; deployment</li> <li>Run <code>./reset.sh</code> to execute the full workflow automatically. The script deletes and recreates the Kind cluster, rebuilds services, builds and loads Docker images, installs/updates the NGINX ingress controller, deploys the Helm chart, waits for pods, and runs smoke tests from inside the cluster.</li> </ol>"},{"location":"11-release1-spec.html#11-acceptance-criteria","title":"11. Acceptance Criteria","text":"<p>Release 1 is considered correctly implemented when:</p> <ul> <li>Both services compile and pass tests.</li> <li>Docker images can be built using the provided Dockerfiles.</li> <li>Kubernetes manifests apply cleanly in the telemetry namespace.</li> <li>Oracle XE is reachable and device_status_current exists.</li> <li>A <code>POST /telemetry</code> followed by <code>GET /devices/{id}/status</code> works end-to-end.</li> <li><code>/actuator/health</code> returns UP for both services.</li> <li><code>/actuator/prometheus</code> is exposed and scrapes metrics for future Prometheus usage.</li> </ul> <p>End of specification.</p>"},{"location":"12-troubleshooting.html","title":"\ud83e\uddf1 1. Deployment &amp; Setup","text":"Command / Config Purpose What to Look For <code>ORACLE_JDBC_URL=jdbc:oracle:thin:@//oracle-db.telemetry.svc.cluster.local:1521/XEPDB1</code> Correct JDBC URL (uses <code>@//</code> for service name). Wrong prefix (<code>@</code> only) causes <code>ORA-12514</code> / <code>ORA-12505</code>. <code>kind create cluster --name telemetry --config kind-config.yaml</code> Create local Kind cluster (ports 80/443 open for ingress). <code>kind get clusters</code> \u2192 <code>telemetry</code>; <code>kubectl cluster-info</code> healthy. <code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml</code> Install NGINX Ingress Controller into Kind. <code>kubectl get pods -n ingress-nginx</code> \u2192 <code>ingress-nginx-controller</code> 1/1 Running. <code>./gradlew clean build</code> Build Java microservices (<code>agent-ingest-svc</code>, <code>device-state-svc</code>). <code>BUILD SUCCESSFUL</code>; JARs under <code>build/libs</code>. <code>docker build -t telemetry/&lt;svc&gt;:local services/&lt;svc&gt;</code> Build Docker images for each service. Image appears in <code>docker images</code>. <code>kind load docker-image telemetry/&lt;svc&gt;:local --name telemetry</code> Load built image into Kind\u2019s internal registry. \u201cImage loaded successfully\u201d; visible in <code>kubectl describe pod</code>. <code>kubectl create namespace telemetry</code> Create namespace for all resources. Listed in <code>kubectl get ns</code>. <code>kubectl create secret generic oracle-credentials --from-literal=ORACLE_USER=telemetry --from-literal=ORACLE_PASSWORD=telemetry_pw --from-literal=ORACLE_JDBC_URL=jdbc:oracle:thin:@//oracle-db.telemetry.svc.cluster.local:1521/XEPDB1 -n telemetry</code> Store Oracle credentials. Secret created successfully. <code>kubectl apply -f oracle-db.yaml</code> / <code>oracle.yml</code> Deploy Oracle manually. <code>oracle-db-0</code> pod created, Ready 1/1. <code>helm upgrade --install oracle . -n telemetry</code> Install Oracle via Helm chart. Successful upgrade/install message. <code>helm upgrade --install telemetry-platform infra/helm/telemetry-platform -n telemetry --create-namespace</code> Deploy full stack (Oracle + services + ingress). <code>helm ls</code> \u2192 <code>STATUS: deployed</code>; pods Ready. <code>kubectl get pods -n telemetry -w</code> Watch rollout live. Pods go <code>Pending \u2192 Running \u2192 Ready</code>. <code>kubectl get svc -n telemetry</code> List cluster services. <code>oracle-db</code> \u2192 <code>1521/TCP</code>. <code>kubectl get ingress -n telemetry</code> Verify ingress resource. Host <code>telemetry.internal</code>, correct paths. <code>kubectl get pvc -n telemetry</code> Check persistent volumes. <code>oracle-data-oracle-db-0</code> \u2192 <code>Bound</code>. <code>kubectl get endpoints -n telemetry oracle-db</code> Validate DB service endpoints. IP:1521 visible; empty = listener down. <code>kubectl rollout status deployment/&lt;svc&gt; -n telemetry</code> Wait for service rollout. \u201csuccessfully rolled out\u201d. <code>kubectl port-forward svc/agent-ingest-svc -n telemetry 8080:8080</code> Test app locally. <code>curl http://localhost:8080/actuator/health</code> \u2192 <code>{\"status\":\"UP\"}</code>. <code>kubectl port-forward svc/oracle-db 1521:1521 -n telemetry</code> Test Oracle locally. Connect via <code>jdbc:oracle:thin:@//localhost:1521/XEPDB1</code>. <code>helm list -n telemetry</code> List Helm releases. <code>telemetry-platform</code> \u2192 <code>deployed</code>. <code>kubectl get events -n telemetry --sort-by=.metadata.creationTimestamp</code> Review recent namespace events. Errors, restarts, or CrashLoops."},{"location":"12-troubleshooting.html#2-debugging-inspection","title":"\ud83e\ude7a 2. Debugging &amp; Inspection","text":"Command Purpose What to Look For <code>kubectl logs -f oracle-db-0 -n telemetry</code> Stream Oracle startup logs. <code>DATABASE IS READY TO USE!</code> or <code>Listening on: ... PORT=1521</code>. <code>kubectl logs -f agent-ingest-0 -n telemetry</code> Watch microservice logs for DB errors. <code>ORA-12514</code>, <code>ORA-12505</code>, <code>ORA-28009</code>, or \u201cConnection refused\u201d. <code>kubectl describe pod oracle-db-0 -n telemetry</code> Full pod info (events, exit codes, restarts). <code>Exit Code 137</code> (OOMKilled), <code>57</code> (Oracle internal). <code>kubectl logs oracle-db-0 -n telemetry --previous</code> View logs from last crash. Fatal ORA errors before restart. <code>kubectl exec -it oracle-db-0 -n telemetry -- bash -lc \"tail -n 100 /opt/oracle/diag/rdbms/*/*/trace/alert_*.log\"</code> Inspect Oracle alert log. ORA-xxxx for memory, file, or startup errors. <code>kubectl exec -it -n telemetry deploy/agent-ingest-svc -- printenv | grep ORACLE_</code> View env vars inside container. All <code>ORACLE_*</code> vars set correctly. <code>kubectl get secret -n telemetry oracle-credentials -o jsonpath='{.data.ORACLE_JDBC_URL}' | base64 -d</code> Decode JDBC URL from secret. Correct host, port, and DB name. <code>kubectl exec -it -n telemetry deploy/agent-ingest-svc -- sh -c \"echo &gt; /dev/tcp/oracle-db.telemetry.svc.cluster.local/1521\"</code> Check Oracle port reachability. Exit 0 = connection OK. <code>kubectl run testbox --rm -it --image=busybox -n telemetry -- /bin/sh</code> \u2192 <code>nc -vz oracle-db.telemetry.svc.cluster.local 1521</code> Network test via netcat. \u201csucceeded/open\u201d = OK. <code>kubectl exec -it oracle-db-0 -n telemetry -- bash</code> \u2192 <code>lsnrctl status | grep XEPDB1</code> Check listener registration. Service XEPDB1 listed. <code>sqlplus sys/$ORACLE_PASSWORD@//localhost/XEPDB1 as sysdba</code> Manual Oracle connection. \u201cConnected to:\u201d output. <code>kubectl get endpoints -n telemetry &lt;svc&gt;</code> Verify service backends. IP:PORT displayed. <code>kubectl get pvc -n telemetry</code> / <code>kubectl describe pvc &lt;pvc&gt;</code> Inspect volume state. <code>Bound</code> = healthy; <code>Pending</code> = storage issue. <code>helm get manifest telemetry-platform -n telemetry</code> See rendered Helm YAML. Verify PVCs, envs, selectors. <code>helm list -n telemetry</code> Confirm Helm release status. <code>STATUS: deployed</code>. <code>kubectl describe statefulset oracle-db -n telemetry</code> Inspect StatefulSet details. Check <code>resources:</code> and <code>volumeClaimTemplates:</code>. <code>kubectl describe statefulset oracle-db -n telemetry | grep -A5 \"resources:\"</code> Review resource limits. Matches Helm values. <code>kubectl get pods -n telemetry -w</code> Watch pod transitions. Running \u2192 Ready. <code>kubectl get configmap -n telemetry</code> Check Helm-injected configs. Expected key-value pairs. <code>kubectl exec -it &lt;pod&gt; -n telemetry -- /bin/bash</code> Enter container shell. Inspect local logs/configs manually. <code>curl -v http://telemetry.internal/telemetry</code> Full ingress path test. <code>200/202</code> = success; <code>503</code> = backend not ready."},{"location":"12-troubleshooting.html#3-troubleshooting-maintenance","title":"\ud83e\uddef 3. Troubleshooting &amp; Maintenance","text":"Command Purpose What to Look For / Expected Outcome <code>kubectl rollout restart deploy/&lt;service&gt; -n telemetry</code> Restart deployments after config changes. Pods recreate \u2192 1/1 Ready. <code>kubectl delete pod &lt;pod&gt; -n telemetry</code> Restart a stuck pod manually. New pod starts cleanly. <code>kubectl scale statefulset oracle-db -n telemetry --replicas=0</code> \u2192 <code>--replicas=1</code> Safely restart Oracle. Pod restarts with new uptime. <code>kubectl delete statefulset oracle-db -n telemetry</code> Remove faulty Oracle instance. StatefulSet gone, ready to redeploy. <code>kubectl delete pvc oracle-data-oracle-db-0 -n telemetry</code> Reset Oracle data. PVC deleted \u2192 new DB on redeploy. <code>kubectl patch pvc oracle-data-oracle-db-0 -n telemetry -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge</code> Remove stuck PVC. PVC disappears. <code>kubectl patch svc oracle-db -n telemetry -p '{\"spec\":{\"selector\":{\"app\":\"oracle-db\"}}}'</code> Fix bad Service selector. <code>kubectl get endpoints oracle-db</code> \u2192 IP:1521. <code>kubectl logs -f oracle-db-0 -n telemetry (after redeploy)</code> Verify successful DB init. <code>######################### DATABASE IS READY TO USE! #########################</code>. <code>docker restart</code> / <code>kind delete cluster &amp;&amp; kind create cluster</code> Clean semaphores / reset environment. Fixes ORA-01081 errors. <code>kubectl apply -f oracle.yml</code> (add <code>/dev/shm</code> volume) Increase Oracle shared memory. Startup memory errors resolved. <code>kubectl delete namespace telemetry --grace-period=0 --force</code> Wipe namespace completely. Namespace removed. <code>helm uninstall telemetry-platform -n telemetry</code> Remove Helm release. Helm reports release deleted. <code>kind delete cluster --name telemetry</code> Destroy Kind cluster. \u201cDeleted clusters: telemetry\u201d. <code>kubectl run telemetry-smoke-test --rm -i --image=curlimages/curl:8.10.1 -n telemetry --command -- sh -c 'curl -sf http://agent-ingest-svc:8080/actuator/health'</code> In-cluster health check. <code>{\"status\":\"UP\"}</code>. <code>curl -v http://localhost:8080/telemetry -H \"Content-Type: application/json\" -d '{...}'</code> Manual POST telemetry test. HTTP 202 success, logs show data ingestion."},{"location":"12-troubleshooting.html#4-quick-symptom-guide","title":"\ud83e\udde9 4. Quick Symptom Guide","text":"Symptom Root Cause Fix <code>ORA-12514 / ORA-12505</code> Wrong JDBC URL (<code>@</code> instead of <code>@//</code>). Use correct format: <code>jdbc:oracle:thin:@//host:1521/XEPDB1</code>. <code>ORA-12541: No listener</code> Listener not started or Service has no endpoints. Restart Oracle; fix Service selector. <code>ORA-01017: Invalid username/password</code> Wrong credentials or missing user. <code>CREATE USER telemetry IDENTIFIED BY telemetry_pw;</code> update secret. <code>ORA-00942: Table or view does not exist</code> Schema missing or migrations not run. Add Flyway scripts (<code>V1__create_table.sql</code>). <code>ORA-01081</code> Stale IPC/semaphore state. Restart cluster or container. <code>CrashLoopBackOff</code> Repeated startup failures. Check logs; verify memory, DB, env vars. <code>Exit Code 137 / OOMKilled</code> Insufficient memory. Increase pod/node memory. <code>503 Service Temporarily Unavailable</code> Ingress with no ready endpoints. Fix DB connection; pods become Ready. <code>Connection refused</code> Network or port mapping issue. Verify ingress/port-forward. \u201c<code>DATABASE IS READY TO USE!</code>\u201d Oracle startup complete. Safe to connect via JDBC."},{"location":"12-troubleshooting.html#quick-checklist-for-contributors","title":"\u2705 Quick Checklist for Contributors","text":"<ul> <li>Pods Pending / CrashLoopBackOff \u2192 <code>kubectl describe pod</code> + <code>kubectl logs</code>.</li> <li>PVC Pending \u2192 Check <code>WaitForFirstConsumer</code> or storage class; delete/recreate.</li> <li>Oracle stuck on \u201cBreak signaled\u201d \u2192 Delete PVC \u2192 restart StatefulSet.</li> <li>No response from services \u2192 <code>kubectl port-forward</code> or in-cluster curl.</li> <li>Full redeploy \u2192 Run <code>kind delete cluster &amp;&amp; kind create cluster</code>.</li> </ul>"},{"location":"12-troubleshooting.html#healthy-system-indicators","title":"\ud83e\udde0 Healthy System Indicators","text":"<p>Oracle logs: <pre><code>######################### DATABASE IS READY TO USE! #########################\n</code></pre></p> <p>Microservice health: <pre><code>curl http://telemetry.internal/telemetry\nHTTP 202 / {\"status\":\"UP\"}\n</code></pre></p> <p>At that point \u2014 everything is configured, connected, and operational.</p>"},{"location":"13-terragrunt-deployment.html","title":"Local Development Guide: Running Telemetry Platform on Kind","text":"<p>This guide explains how a new developer can deploy the Telemetry Platform locally using:</p> <ul> <li>Kind (Kubernetes-in-Docker)</li> <li>Terragrunt</li> <li>Helm</li> <li>Local Docker images</li> </ul> <p>By the end you will have a full local environment running:</p> <ul> <li>Oracle XE</li> <li>agent-ingest service</li> <li>device-state service</li> <li>ingress-nginx</li> <li>the telemetry gateway at: <code>http://telemetry.internal/telemetry</code></li> </ul>"},{"location":"13-terragrunt-deployment.html#prerequisites","title":"Prerequisites","text":"<p>Install the following tools:</p> Tool Version Notes Docker Desktop latest Linux containers required Kind \u2265 0.20 kind cluster runner Kubectl \u2265 1.25 Kubernetes CLI Terraform \u2265 1.5 indirectly used via Terragrunt Terragrunt \u2265 0.93 runs Terraform modules Helm \u2265 3.x chart installer Gradle included via wrapper build services"},{"location":"13-terragrunt-deployment.html#1-build-and-load-local-images","title":"1. Build and Load Local Images","text":"<p>First, build the services and load their Docker images into the Kind cluster.</p>"},{"location":"13-terragrunt-deployment.html#11-build-java-services","title":"1.1 Build Java services","text":"<p>From project root:</p> <pre><code>./gradlew :agent-ingest-svc:build :device-state-svc:build\n</code></pre>"},{"location":"13-terragrunt-deployment.html#12-build-docker-images","title":"1.2 Build Docker images","text":"<pre><code>docker build -t telemetry/agent-ingest-svc:local agent-ingest-svc\ndocker build -t telemetry/device-state-svc:local device-state-svc\n</code></pre> <p>Note: These tags must match what the Helm chart expects.</p>"},{"location":"13-terragrunt-deployment.html#13-load-images-into-kind-after-cluster-exists","title":"1.3 Load images into Kind (after cluster exists)","text":"<p>Once the Kind cluster is created (next section), run:</p> <pre><code>kind load docker-image telemetry/agent-ingest-svc:local --name telemetry\nkind load docker-image telemetry/device-state-svc:local --name telemetry\n</code></pre>"},{"location":"13-terragrunt-deployment.html#2-deploy-the-local-kind-environment-using-terragrunt","title":"2. \ud83d\udea2 Deploy the Local Kind Environment Using Terragrunt","text":"<p>All infrastructure components are managed through <code>infra/live/dev</code>.</p> <p>The environment consists of:</p> <ul> <li>Kind cluster (with ingress host port mappings)</li> <li>Ingress-nginx (Kind provider variant)</li> <li>Telemetry Platform Helm chart</li> </ul> <p>We apply each unit explicitly.</p>"},{"location":"13-terragrunt-deployment.html#21-create-the-kind-cluster","title":"2.1 Create the Kind Cluster","text":"<pre><code>cd infra/live/dev/platform/kind-cluster\nterragrunt apply\n</code></pre> <p>This:</p> <ul> <li>Creates the Kind cluster <code>telemetry</code></li> <li>Adds the <code>ingress-ready=true</code> label</li> <li>Maps host ports 80 \u2192 80 and 443 \u2192 443</li> </ul> <p>Verify:</p> <pre><code>kind get clusters\nkubectl get nodes\nkubectl config get-contexts | grep kind-telemetry\n</code></pre>"},{"location":"13-terragrunt-deployment.html#22-deploy-ingress-nginx","title":"2.2 Deploy ingress-nginx","text":"<pre><code>cd ../ingress-nginx\nterragrunt apply\n</code></pre> <p>Verify:</p> <pre><code>kubectl get pods -n ingress-nginx\nkubectl get ingressclass\n</code></pre>"},{"location":"13-terragrunt-deployment.html#23-deploy-the-telemetry-platform-helm-release","title":"2.3 Deploy the telemetry platform Helm release","text":"<pre><code>cd ../telemetry-platform\nterragrunt apply\n</code></pre> <p>This installs:</p> <ul> <li>Oracle DB</li> <li>agent-ingest-svc</li> <li>device-state-svc</li> <li>Ingress with host: <code>telemetry.internal</code></li> </ul>"},{"location":"13-terragrunt-deployment.html#3-verification-checklist","title":"3. \ud83d\udd0d Verification Checklist","text":""},{"location":"13-terragrunt-deployment.html#31-namespace-exists","title":"3.1 Namespace exists","text":"<pre><code>kubectl get ns\n</code></pre> <p>Expected: <code>telemetry</code> and <code>ingress-nginx</code></p>"},{"location":"13-terragrunt-deployment.html#32-pods-are-running","title":"3.2 Pods are running","text":"<pre><code>kubectl get pods -n telemetry\n</code></pre> <p>Expected:</p> <pre><code>agent-ingest-svc-xxxxx     Running\nagent-ingest-svc-yyyyy     Running\ndevice-state-svc-zzzzz     Running\noracle-db-0                Running\n</code></pre>"},{"location":"13-terragrunt-deployment.html#33-ingress-is-configured","title":"3.3 Ingress is configured","text":"<pre><code>kubectl get ingress -n telemetry\n</code></pre> <p>Expected:</p> <pre><code>telemetry-gateway   telemetry.internal   80\n</code></pre>"},{"location":"13-terragrunt-deployment.html#34-add-host-entry-only-once","title":"3.4 Add host entry (only once)","text":"<p>Edit <code>/etc/hosts</code>:</p> <pre><code>echo \"127.0.0.1 telemetry.internal\" | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"13-terragrunt-deployment.html#35-end-to-end-test","title":"3.5 End-to-end test","text":"<pre><code>curl -v http://telemetry.internal/telemetry \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"deviceId\": \"local-test\",\n        \"cpu\": 0.5,\n        \"mem\": 0.2,\n        \"diskAlert\": false,\n        \"timestamp\": \"2025-11-02T18:22:00Z\",\n        \"processes\": []\n      }'\n</code></pre> <p>Expected:</p> <pre><code>HTTP/1.1 202 Accepted\n</code></pre>"},{"location":"13-terragrunt-deployment.html#4-troubleshooting","title":"4. Troubleshooting","text":""},{"location":"13-terragrunt-deployment.html#imagepullbackoff-errimagepull","title":"ImagePullBackOff / ErrImagePull","text":"<p>Cause: images were not loaded into Kind.</p> <p>Fix:</p> <pre><code>kind load docker-image telemetry/agent-ingest-svc:local --name telemetry\nkind load docker-image telemetry/device-state-svc:local --name telemetry\nkubectl rollout restart deployment/agent-ingest-svc -n telemetry\nkubectl rollout restart deployment/device-state-svc -n telemetry\n</code></pre>"},{"location":"13-terragrunt-deployment.html#curl-connection-refused-for-telemetryinternal","title":"curl: Connection refused for telemetry.internal","text":"<p>Check:</p> <ol> <li>Is ingress-nginx running?</li> </ol> <pre><code>kubectl get pods -n ingress-nginx\n</code></pre> <ol> <li>Does <code>/etc/hosts</code> contain:</li> </ol> <pre><code>127.0.0.1 telemetry.internal\n</code></pre> <ol> <li>Is Kind mapping hostPort 80 \u2192 container?</li> </ol> <pre><code>docker inspect telemetry-control-plane | grep '\"80/tcp\"'\n</code></pre>"},{"location":"13-terragrunt-deployment.html#terraformterragrunt-error-context-kind-telemetry-does-not-exist","title":"terraform/terragrunt error: context \"kind-telemetry\" does not exist","text":"<p>Cause: You deleted the cluster but ran <code>terragrunt plan</code> before recreating it.</p> <p>Fix: Recreate cluster first:</p> <pre><code>cd infra/live/dev/platform/kind-cluster\nterragrunt apply\n</code></pre>"},{"location":"13-terragrunt-deployment.html#helm-release-stuck-on-still-creating","title":"Helm release stuck on Still creating...","text":"<p>Usually caused by image pull failures.</p> <p>Check:</p> <pre><code>kubectl get pods -n telemetry\n</code></pre> <p>If any pod is not Running, fix the pod issue (image, env, container port), then rerun:</p> <pre><code>terragrunt apply\n</code></pre>"},{"location":"13-terragrunt-deployment.html#ingress-controller-pending","title":"Ingress controller Pending","text":"<p>Cause: missing node label <code>ingress-ready=true</code>.</p> <p>Fix:</p> <pre><code>kubectl label node telemetry-control-plane ingress-ready=true --overwrite\n</code></pre>"},{"location":"13-terragrunt-deployment.html#unable-to-resolve-telemetryinternal","title":"Unable to resolve telemetry.internal","text":"<p>Check DNS/hosts:</p> <pre><code>dig telemetry.internal\n</code></pre> <p>If empty:</p> <pre><code>echo \"127.0.0.1 telemetry.internal\" | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"13-terragrunt-deployment.html#5-appendix-all-commands-used-in-this-guide","title":"5. Appendix \u2014 All Commands Used in This Guide","text":"<p>Below is every command referenced throughout this onboarding process, grouped by purpose.</p>"},{"location":"13-terragrunt-deployment.html#kind-cluster-management","title":"Kind Cluster Management","text":"<pre><code>kind get clusters\nkind delete cluster --name telemetry\nkind create cluster --config kind-config.yaml --name telemetry\n</code></pre>"},{"location":"13-terragrunt-deployment.html#terragrunt-deployment","title":"Terragrunt Deployment","text":"<p>Create cluster:</p> <pre><code>cd infra/live/dev/platform/kind-cluster\nterragrunt apply\n</code></pre> <p>Install ingress-nginx:</p> <pre><code>cd ../ingress-nginx\nterragrunt apply\n</code></pre> <p>Install telemetry platform:</p> <pre><code>cd ../telemetry-platform\nterragrunt apply\n</code></pre> <p>Run full stack plan (safe):</p> <pre><code>cd infra/live/dev\nterragrunt run --all plan\n</code></pre> <p>Note: Do not run <code>terragrunt run --all apply</code> unless the whole environment is idempotent.</p>"},{"location":"13-terragrunt-deployment.html#docker-image-build-load","title":"Docker Image Build + Load","text":"<pre><code>./gradlew :agent-ingest-svc:build :device-state-svc:build\n\ndocker build -t telemetry/agent-ingest-svc:local agent-ingest-svc\ndocker build -t telemetry/device-state-svc:local device-state-svc\n\nkind load docker-image telemetry/agent-ingest-svc:local --name telemetry\nkind load docker-image telemetry/device-state-svc:local --name telemetry\n</code></pre>"},{"location":"13-terragrunt-deployment.html#kubernetes-debugging","title":"Kubernetes Debugging","text":"<p>Pods:</p> <pre><code>kubectl get pods -n telemetry\nkubectl describe pod &lt;pod&gt; -n telemetry\nkubectl logs &lt;pod&gt; -n telemetry --all-containers=true\n</code></pre> <p>Services, deployments, events:</p> <pre><code>kubectl get all -n telemetry\nkubectl get events -n telemetry --sort-by=.lastTimestamp\nkubectl get ingress -n telemetry\n</code></pre> <p>Rollout restart:</p> <pre><code>kubectl rollout restart deployment/&lt;name&gt; -n telemetry\n</code></pre>"},{"location":"13-terragrunt-deployment.html#ingress-dns","title":"Ingress + DNS","text":"<pre><code>kubectl get pods -n ingress-nginx\nkubectl get ingressclass\nkubectl describe ingress telemetry-gateway -n telemetry\n</code></pre> <p>Add host entry:</p> <pre><code>echo \"127.0.0.1 telemetry.internal\" | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"13-terragrunt-deployment.html#api-testing","title":"API Testing","text":"<pre><code>curl -v http://telemetry.internal/telemetry \\\n  -H \"Content-Type: application/json\" \\\n  -d '{ ... }'\n</code></pre>"}]}